{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45225d96ef8db436",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# ML Fundamentals Checkpoint\n",
    "\n",
    "This checkpoint is designed to test your understanding of the content from the Machine Learning Fundamentals Cumulative Lab.\n",
    "\n",
    "Specifically, this will cover:\n",
    "\n",
    "* Performing a train-test split to evaluate model performance on unseen data\n",
    "* Applying appropriate preprocessing steps to training and test data\n",
    "* Identifying overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80e2de554ec6f219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Your Task: Build and Evaluate a Ridge Regression Model to Predict Home Prices\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "You will be using the Ames Housing dataset, modeling the `SalePrice` based on all other numeric features of the dataset. You can view the `data_description.txt` file for explanations of these variables if desired, but the specific feature descriptions can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0eab44802b65ea1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "# Read in CSV file\n",
    "df = pd.read_csv(\"ames.csv\", index_col=0)\n",
    "# Keep only numeric data\n",
    "df = df.select_dtypes(include=\"number\")\n",
    "# Keep only columns with no missing values\n",
    "df = df.dropna(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-927990ad603957cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Modeling\n",
    "\n",
    "You will apply a **predictive** modeling process using scikit-learn. That means that you are trying to build a model with the best performance on predicting the target (`SalePrice`) using the features of unseen data.\n",
    "\n",
    "For this reason you will first perform a **train-test split**, so that you are fitting the model using the training dataset and evaluating the model using the testing dataset.\n",
    "\n",
    "You will also report model **metrics** in terms of both r-squared and RMSE, for both the train and test data, in order to interpret your model performance.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### 1. Perform a Train-Test Split\n",
    "\n",
    "#### 2. Preprocess Data\n",
    "\n",
    "#### 3. Fit a `Ridge` Model\n",
    "\n",
    "#### 4. Evaluate the Model Performance\n",
    "\n",
    "#### 5. Interpret the Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5667f384accd25f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Perform a Train-Test Split\n",
    "\n",
    "This step has two parts. First, separate `df` into `X` and `y`.\n",
    "\n",
    "* `X` should be a pandas `DataFrame` containing all columns of `df` except for the target\n",
    "* `y` should be a pandas `Series` containing just the target\n",
    "\n",
    "The target is `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4893c1f1208ec61",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step1.1\n",
    "# Replace None with appropriate code\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the type and shape of X\n",
    "assert type(X) == pd.DataFrame\n",
    "assert X.shape == (1460, 33)\n",
    "\n",
    "# Checking the type and shape of y\n",
    "assert type(y) == pd.Series\n",
    "assert y.shape == (1460,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ccf555ce3e71f36d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have `X` and `y`, perform a train-test split ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). Let's say that 40% of the data should be in the test set (60% in the train set), and the random state should be 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a2f9098bf45328b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step1.2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform train-test split. Replace None with the function and appropirate arguements\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shapes\n",
    "# (If this fails, make sure you specified the appropriate test_size)\n",
    "assert X_train.shape == (876, 33)\n",
    "assert y_train.shape == (876,)\n",
    "\n",
    "# Checking the contents\n",
    "# (If this fails, make sure you specified the appropriate random_state)\n",
    "assert X_train.iloc[100][\"YearBuilt\"] == 1947\n",
    "assert y_train.iloc[100] == 110000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2324103f0af706e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Preprocess Data\n",
    "\n",
    "### Ridge Regression Recap\n",
    "\n",
    "We are going to use a `Ridge` regression, which adds a penalty term to the square of the magnitude of the coefficients.\n",
    "\n",
    "In other words, whereas an ordinary least squares regression uses this cost function:\n",
    "\n",
    "$$ \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} ) -b )^2$$\n",
    "\n",
    "...where $n$ is the number of rows in the dataset, $y$ is the target value, $k$ is the number of features in the dataset, $m$ is the slope parameter (coefficient) we are trying to find, $x$ is the value of the feature, and $b$ is the intercept...\n",
    "\n",
    "...a ridge regression uses this cost function:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "The difference is the $\\lambda \\sum_{j=1}^p m_j^2$ at the end, where $\\lambda$ is a _hyperparameter_ that we specify, which is multiplied by the coefficients. **The goal of fitting a model is finding $m$ values to *minimize* the cost function**, so a larger $\\lambda$ means more of a penalty on high coefficients. This is a form of *regularization* that should help with overfitting.\n",
    "\n",
    "### Scaling\n",
    "\n",
    "Ridge regression, which uses L2 norm regularization, means that feature values need to be standardized so that some values aren't penalized \"unfairly\". So let's go ahead and apply a `StandardScaler` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) to the entire feature set, fitting and transforming `X_train` and transforming `X_test`.\n",
    "\n",
    "Create new variables `X_train_scaled` and `X_test_scaled` which are the scaled versions of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec2da7571019a17a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import StandardScaler\n",
    "None\n",
    "\n",
    "# Instantiate a scaler object\n",
    "scaler = None\n",
    "\n",
    "# Fit the scaler on X_train and then transform X_train\n",
    "None\n",
    "X_train_scaled = None\n",
    "\n",
    "# Transform X_test\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# scaler should be a StandardScaler\n",
    "assert type(scaler) == StandardScaler\n",
    "# scaler should be fitted\n",
    "assert type(scaler.mean_) == np.ndarray\n",
    "\n",
    "# X_train_scaled should have the same shape\n",
    "# as X_train but with different contents\n",
    "assert X_train_scaled.shape == X_train.shape\n",
    "assert X_train_scaled[0][0] != X_train.iloc[0].iloc[0]\n",
    "\n",
    "# Same goes for X_test_scaled\n",
    "assert X_test_scaled.shape == X_test.shape\n",
    "assert X_test_scaled[0][0] != X_test.iloc[0].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b1011f4acfaeec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Fit a `Ridge` Model\n",
    "\n",
    "Now that we have our preprocessed data, fit a `Ridge` regression model ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)).\n",
    "\n",
    "When instantiating the model, specify an `alpha` (regularization penalty) of 100, a `solver` of `\"sag\"` (stochastic average gradient descent), and a `random_state` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45802a6f1fb065c0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step3\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import Ridge model from scikit-learn\n",
    "None\n",
    "\n",
    "# Instantiate the model\n",
    "model = None\n",
    "\n",
    "# Fit the model on the scaled training data\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model should be a Ridge regressor\n",
    "assert type(model) == Ridge\n",
    "\n",
    "# model should be fitted\n",
    "assert type(model.coef_) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-593a8cb979622402",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Evaluate the Model Performance\n",
    "\n",
    "First, use the fitted model to generate `SalePrice` predictions for both the train and the test data. These variables should be called `y_pred_train` for the training data and `y_pred_test` for the testing data.\n",
    "\n",
    "Make sure you use the scaled versions of the data!\n",
    "\n",
    "We will use these predictions to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bd372462ff071aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step4.1\n",
    "# Replace None with appropriate code\n",
    "y_pred_train = None\n",
    "y_pred_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both should be NumPy arrays\n",
    "assert type(y_pred_train) == np.ndarray\n",
    "assert type(y_pred_test) == np.ndarray\n",
    "\n",
    "# Should have the same shapes as y_train and y_test, respectively\n",
    "assert y_pred_train.shape == y_train.shape\n",
    "assert y_pred_test.shape == y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eb5aa46c99fc351c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, use those predicted values to evaluate the model in terms of both:\n",
    "\n",
    "* RMSE, using `mean_squared_error` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)) with `squared=False`\n",
    "* R-squared, using `r2_score` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html))\n",
    "\n",
    "Apply these to both the train and test datasets. We have already imported the necessary functions; you just need to call the functions and pass in the appropriate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7689e4d208bcbff6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step4.2\n",
    "# Replace None with appropriate code\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "train_rmse = None\n",
    "test_rmse = None\n",
    "\n",
    "train_r2 = None\n",
    "test_r2 = None\n",
    "\n",
    "print(f\"\"\"\n",
    "RMSE\n",
    "Train: {train_rmse} \\t Test: {test_rmse}\n",
    "\n",
    "R-squared\n",
    "Train: {train_r2} \\t Test: {test_r2}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE scores should be floating point numbers\n",
    "assert type(train_rmse) == np.float64 or type(train_rmse) == float\n",
    "assert type(test_rmse) == np.float64 or type(test_rmse) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared scores should be floating point numbers\n",
    "assert type(train_r2) == np.float64 or type(train_r2) == float\n",
    "assert type(test_r2) == np.float64 or type(test_r2) == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0a2420444cac43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Interpret the Model Performance\n",
    "\n",
    "Here's we'll focus on RMSE metrics, since those can be represented as \"the average error in the price prediction\".\n",
    "\n",
    "Recall that the purpose of using regularization (e.g. a `Ridge` model) is to reduce overfitting.\n",
    "\n",
    "Let's say that we previously used a basic ordinary least squares regression model, and we got RMSE scores of `$33,633.14` on the training data and `$39,255.80` on the test data. A full comparison of scores is in the table below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b6d0c33512bd6e89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "scores = pd.DataFrame([\n",
    "    {\"Model\": \"Linear Regression\", \"Train RMSE\": 33633.14, \"Test RMSE\": 39255.80},\n",
    "    {\"Model\": \"Ridge Regression\", \"Train RMSE\": 33910.84, \"Test RMSE\": 39213.66},\n",
    "])\n",
    "scores.set_index(\"Model\", inplace=True)\n",
    "scores.style.format(\"${:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a8a4cc0ff171ee2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Was our strategy of using a `Ridge` model to reduce overfitting successful? Which model is better?\n",
    "\n",
    "Assign the variable `best_model_name` to either `\"Linear Regression\"` or `\"Ridge Regression\"`.\n",
    "\n",
    "Recall that this is a predictive modeling context, so when we are defining the \"best\" model, we are interested in the model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42f97ee6482596fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step5\n",
    "# Replace None with appropriate code\n",
    "best_model_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be \"Linear Regression\" or \"Ridge Regression\"\n",
    "assert best_model_name in [\"Linear Regression\", \"Ridge Regression\"]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
