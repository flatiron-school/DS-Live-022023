{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34139fb74befcf21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Natural Language Processing Checkpoint\n",
    "This checkpoint is designed to test your understanding of the content from the Text Classification Cumulative Lab. \n",
    "\n",
    "Specifically, this will cover:\n",
    "\n",
    "- Preprocessing and exploring text data using `nltk`\n",
    "- Vectorizing text data using a bag-of-words approach\n",
    "- Fitting machine learning models using vectorized text data\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "In this repository under the file path `data/movie_descriptions.csv` there is a CSV file containing the titles, genres, and descriptions for 5,000 films pulled from [IMDb](https://www.kaggle.com/hijest/genre-classification-dataset-imdb).\n",
    "\n",
    "**The features of interest for this analysis will be:**\n",
    "\n",
    "1. `desc`: The description of the film, which we will explore and then use as the features of our model\n",
    "2. `genre`: The target for our predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:51:16.295737Z",
     "start_time": "2021-11-08T18:51:15.844207Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-281bb10d1f157ca2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "# Import the data\n",
    "data = pd.read_csv('movie_descriptions.csv')\n",
    "\n",
    "# Output a sample\n",
    "data = data.sample(1500, random_state=100)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f04cd94df7c7b107",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "data.genre.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-402a4b03e41919f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "1. Initialize tokenizer and stemmer objects to prepare for text preprocessing\n",
    "2. Write a function that implements standard \"bag of words\" text preprocessing\n",
    "3. Initialize and fit a `CountVectorizer` from `sklearn`\n",
    "3. Vectorize data using `CountVectorizer`\n",
    "4. Fit a decision tree classifier on vectorized text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f86b392a061c5b2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Initialize Tokenizer, Stemmer, and Stopwords Objects\n",
    "\n",
    "In our exploratory text analysis, we will:\n",
    "\n",
    "* Standardize case\n",
    "* Tokenize (split text into words)\n",
    "* Remove stopwords\n",
    "* Stem words\n",
    "\n",
    "Three of those steps require that we import some functionality from `nltk`. In the cell below, create:\n",
    "\n",
    "* An instance of `RegexpTokenizer` ([documentation here](https://www.nltk.org/api/nltk.tokenize.regexp.html#module-nltk.tokenize.regexp)) called `tokenizer`\n",
    "  * The regex pattern should select all words with three or more characters. You can use the pattern `r\"(?u)\\w{3,}\"`\n",
    "* A list of stopwords (documentation [here](https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus) and [here](https://www.nltk.org/nltk_data/)) called `stopwords_list`\n",
    "* An instance of `PorterStemmer` ([documentation here](https://www.nltk.org/api/nltk.stem.porter.html)) called `stemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:51:17.616392Z",
     "start_time": "2021-11-08T18:51:16.358327Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac8d9d14c1329b01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step1\n",
    "# Run this line in a new cell if nltk isn't working\n",
    "# !pip install nltk\n",
    "\n",
    "# Replace None with appropriate code\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create an intance of the RegexpTokenizer with the variable name `tokenizer`\n",
    "# The regex pattern should select all words with three or more characters\n",
    "tokenizer = None\n",
    "\n",
    "# Create a list of stopwords in English\n",
    "stopwords_list = None\n",
    "# Create an instance of nltk's PorterStemmer with the variable name `stemmer`\n",
    "stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that variables are no longer None\n",
    "assert tokenizer\n",
    "assert stopwords_list\n",
    "assert stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33101c1955e971d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) Write a Function That Implements Standard Text Preprocessing\n",
    "\n",
    "In the cell below, complete the `preprocess_text` function so the inputted text is returned lower cased, tokenized, stopwords removed, and stemmed.\n",
    "\n",
    "For example, if you input the text\n",
    "\n",
    "```\n",
    "This is an example sentence for preprocessing.\n",
    "```\n",
    "\n",
    "The result of `preprocess_text` should be this list of strings:\n",
    "\n",
    "```python\n",
    "['exampl', 'sentenc', 'preprocess']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step2\n",
    "def preprocess_text(text, tokenizer, stopwords_list, stemmer):\n",
    "    # Standardize case (lowercase the text)\n",
    "    lower_t = None\n",
    "    \n",
    "    # Tokenize text using `tokenizer`\n",
    "    tokens = None\n",
    "    \n",
    "    # Remove stopwords using `stopwords_list`\n",
    "    stopped_tokens = None\n",
    "    \n",
    "    # Stem the tokenized text using `stemmer`\n",
    "    stems = None\n",
    "    \n",
    "    \n",
    "    # Return the preprocessed text\n",
    "    return stems\n",
    "preprocess_text(\"This is an example sentence for preprocessing.\", tokenizer, stopwords_list, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType\n",
    "\n",
    "assert type(preprocess_text) == FunctionType\n",
    "assert type(preprocess_text('Example text', tokenizer, stopwords_list, stemmer)) == list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0897c963ea268a17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that the function has been created, use it to preprocess the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a65bd7ab76cef9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "# (This may take a while due to nested loops)\n",
    "text_data = data.desc.apply(lambda x: preprocess_text(x, tokenizer, stopwords_list, stemmer))\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eea69e9c014d5d8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "data[\"preprocessed_text\"] = text_data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6055dd6b224b8099",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's take a look at the top ten most frequent words for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-89aa21c97d821cb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up figure and axes\n",
    "fig, axes = plt.subplots(nrows=7, figsize=(12, 12))\n",
    "\n",
    "# Empty dict to hold words that have already been plotted and their colors\n",
    "plotted_words_and_colors = {}\n",
    "# Establish color palette to pull from\n",
    "# (If you get an error message about popping from an empty list, increase this #)\n",
    "color_palette = sns.color_palette('cividis', n_colors=38)\n",
    "\n",
    "# Creating a plot for each unique genre\n",
    "data_by_genre = [y for _, y in data.groupby('genre', as_index=False)]\n",
    "for idx, genre_df in enumerate(data_by_genre):\n",
    "    # Find top 10 words in this genre\n",
    "    all_words_in_genre = genre_df.preprocessed_text.explode()\n",
    "    top_10 = all_words_in_genre.value_counts()[:10]\n",
    "    \n",
    "    # Select appropriate colors, reusing colors if words repeat\n",
    "    colors = []\n",
    "    for word in top_10.index:\n",
    "        if word not in plotted_words_and_colors:\n",
    "            new_color = color_palette.pop(0)\n",
    "            plotted_words_and_colors[word] = new_color\n",
    "        colors.append(plotted_words_and_colors[word])\n",
    "    \n",
    "    # Select axes, plot data, set title\n",
    "    ax = axes[idx]\n",
    "    ax.bar(top_10.index, top_10.values, color=colors)\n",
    "    ax.set_title(genre_df.iloc[0].genre.title())\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b9cdaabf9fb047d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Fit a Count Vectorizer\n",
    "\n",
    "Now that we have explored the data some, let's prepare it for modeling.\n",
    "\n",
    "Before we fit a vectorizer to the data, we need to convert the list of tokens for each document back to a string datatype and create a train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:51:40.308407Z",
     "start_time": "2021-11-08T18:51:40.292643Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b51f2230605c794d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert token lists to strings\n",
    "data[\"joined_preprocessed_text\"] = data[\"preprocessed_text\"].str.join(\" \")\n",
    "\n",
    "# Create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[\"joined_preprocessed_text\"], data.genre, test_size=0.3, random_state=2021)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the cell below, create a CountVectorizer instance ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)) with default arguments, called `vectorizer`, and fit it to the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CodeGrade step3\n",
    "# Import the CountVectorizer object from sklearn\n",
    "\n",
    "None\n",
    "\n",
    "# Create a `vectorizer` instance\n",
    "vectorizer = None\n",
    "\n",
    "# Fit the vectorizer to the training data\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-878793ee1cb75b9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4) Vectorize the Data\n",
    "\n",
    "In the cell below, vectorize the training and test datasets using the fitted count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:51:41.047346Z",
     "start_time": "2021-11-08T18:51:40.809618Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2de1876d86b996ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step4\n",
    "# Replace None with appropriate code\n",
    "\n",
    "X_train_vectorized = None\n",
    "X_test_vectorized = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csr import csr_matrix\n",
    "assert type(X_train_vectorized) == csr_matrix\n",
    "assert type(X_test_vectorized) == csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7f79ea442cc186b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5) Fit a Decision Tree Model\n",
    "\n",
    "In the cell below, \n",
    "\n",
    "- Create an instance of `sklearn`'s `DecisionTreeClassifier` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)), set a `random_state` of 42 but otherwise use default arguments, with the variable name `dt`\n",
    "- Fit the model to the vectorized training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:51:42.154158Z",
     "start_time": "2021-11-08T18:51:41.138578Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-149edcbb04ffb6c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# CodeGrade step5\n",
    "# Replace None with appropriate code\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "None\n",
    "\n",
    "# Initialize `dt`\n",
    "dt = None\n",
    "\n",
    "# Fit the model to the training data\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cdaffa8177aaf22f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following code will now evaluate our model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c19dcab44955d73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "plot_confusion_matrix(dt, X_test_vectorized, y_test, ax=ax, cmap=\"cividis\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
